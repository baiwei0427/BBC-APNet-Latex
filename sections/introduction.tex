\vspace{-2mm}
\section{Introduction}\label{sec:intro}
Datacenters applications generate a mix of workloads with both latency-sensitive small messages and throughput-sensitive bulk transfers. Hence, datacenter network (DCN) transport should provide low latency and high throughput simultaneously to meet the requirements of applications.

It is a challenge to achieve both goals that are essentially at odds, especially under the shared \emph{shallow-buffered} commodity switches in production DCNs. This challenge has been identified 7 years ago by Microsoft researchers in their production DCNs. To address it, they leveraged ECN~\cite{ecn} to strike the tradeoff between high throughput and low latency, and showed that a properly configured per-port ECN/RED marking scheme could well utilize the shallow buffer to achieve both high throughput and low latency, while still reserving certain headroom to absorb micro-bursts~\cite{dctcp}. Since then, ECN-based transports become flourishing~\cite{dctcp,d2tcp,tuning,l2dct} and are widely adopted in industry.

However, in this paper, we show that this seemingly solved problem resurges and the solution is now being re-challenged, due to the recent industrial trend. The link speed of production DCNs is growing fast from 1Gbps to 100Gbps, whereas the buffer size of commodity switches increases slowly (\eg, from 4MB at 1Gbps to 16MB at 100Gbps), significantly outpaced by the link speed. Consequently, the buffer size per port per Gbps drops from 85KB to 5.12KB , leading to an \emph{extremely shallow-buffered} DCN environment ($\S$\ref{sec:buffersize}).

We show that it is hard for prior TCP/ECN solutions to remain effective when buffer is extremely shallow ($\S$\ref{sec:problem}). On the one hand, if we configure the ECN marking threshold as originally proposed~\cite{dctcp,tuning}, it causes excessive packet losses even before ECN reacts when many ports are active simultaneously. On the other hand, if we configure a relatively lower ECN threshold than original one, it wastes bandwidth and degrades throughput unnecessarily when fewer ports are busy because ECN over-reacts.

This problem is severe, but receives little attention so far, and there is no readily deployable solution either. Thus, the key contribution of this paper is to expose this problem and its consequences experimentally, and introduce an \emph{extremely simple}, yet effective and readily deployable solution, named \textbf{\sys} (Buffer-aware Congestion Control), to it.

%---essentially, one more ECN configuration is enough!

Our design of \sys is inspired by the understanding of modern switching chip functionalities. We are surprised to find that to solve our problem, \emph{one more ECN configuration is enough}! At its core, \sys inherits the success of per-port ECN/RED by DCTCP~\cite{dctcp}, and further enables shared buffer ECN/RED to cope with the extremely shallow buffer problem. Shared buffer ECN/RED follows the RED~\cite{RED} algorithm but tracks the occupancy of the shared buffer pool to mark packets. Packets get marked if the shared buffer occupancy exceeds pre-defined thresholds. While this function is there~\cite{arista_ecn,mqecn}, it was less understood and seldom used previously in literature. \sys perhaps exploits it for the first time.

In \sys, shared buffer ECN/RED and per-port ECN/RED work complementarily to each other. When fewer ports are active, the shared buffer is abundant. Hence, per-port ECN/RED will take effect first and strike the balance of high throughput and low latency as before~\cite{dctcp}. When more and more ports become active, the shared buffer turns scarcer. Thus, shared buffer ECN/RED will automatically be triggered first to prevent packet losses---\sys trades throughput for latency when achieving both becomes impossible.

%\sys can be realized by one additional command (Figure~\ref{fig:arista_ecn_cmd}). We have implemented \sys and validated its efficacy in a small-scale 100Gbps testbed with Arista 7060CX-32S-F switch ($\S\ref{subsec:experiments}$).

We evaluate the performance of \sys using ns-2 simulations ($\S\ref{sec:evaluation}$). At low loads, \sys fully utilizes the link capacity. It achieves up to 13.5\% lower average completion time for large flows, compared to a conservative ECN configuration. At high loads, \sys keeps low packet loss rate while only sacrificing a small amount of throughput. It achieves up to 94.4\% lower 99th percentile completion time for small flows while only degrading large flows by up to 2.8\%, compared to a standard ECN configuration.

The rest of the paper is organized as follows. We introduce extremely shallow switch buffer and its impacts in $\S\ref{sec:background}$ and $\S\ref{sec:problem}$, respectively. We present the design of \sys in $\S\ref{sec:design}$. $\S\ref{sec:evaluation}$ presents evaluation results. We discuss related work in $\S\ref{sec:related}$ and conclude the paper in $\S\ref{sec:conclusion}$.


